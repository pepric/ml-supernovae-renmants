{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7374e441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n================================================================================\\nFEATURE IMPORTANCE AND OCCURRENCE ANALYSIS\\n================================================================================\\nAuthors: Giuseppe Riccio, Stefano Cavuoti\\nDate: 2026-02-24\\nDescription:\\n    This script analyzes feature importance results generated from machine \\n    learning experiments. It processes CSV files to identify the most relevant \\n    features (Haar or Haralick) across different bands/experiments.\\n    \\n    Key functionalities:\\n    1. Filters features based on importance thresholds.\\n    2. Removes \"Total\" aggregate bands to focus on specific image features.\\n    3. Calculates statistics across multiple experiments (Mean, Best, Worst rank).\\n    4. Computes occurrence frequency and percentage for each feature.\\n    5. Exports detailed ranking and occurrence reports to CSV and TXT files.\\n\\nDependencies:\\n    - numpy\\n    - matplotlib\\n    - pandas\\n    - os\\n    - IPython (for cell magic capture)\\n\\nUsage:\\n    - Set the \\'exp\\' variable to \"haar\" or \"haralick\".\\n    - Ensure the \\'importance_analysis/importance_[exp]/\\' directory exists.\\n    - The script expects CSV files ending with the specified \\'target\\' (e.g., CSM).\\n================================================================================\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "FEATURE IMPORTANCE AND OCCURRENCE ANALYSIS\n",
    "================================================================================\n",
    "Authors: Giuseppe Riccio, Stefano Cavuoti\n",
    "Date: 2026-02-24\n",
    "Description:\n",
    "    This script analyzes feature importance results generated from machine \n",
    "    learning experiments. It processes CSV files to identify the most relevant \n",
    "    features (Haar or Haralick) across different bands/experiments.\n",
    "    \n",
    "    Key functionalities:\n",
    "    1. Filters features based on importance thresholds.\n",
    "    2. Removes \"Total\" aggregate bands to focus on specific image features.\n",
    "    3. Calculates statistics across multiple experiments (Mean, Best, Worst rank).\n",
    "    4. Computes occurrence frequency and percentage for each feature.\n",
    "    5. Exports detailed ranking and occurrence reports to CSV and TXT files.\n",
    "\n",
    "Dependencies:\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - pandas\n",
    "    - os\n",
    "    - IPython (for cell magic capture)\n",
    "\n",
    "Usage:\n",
    "    - Set the 'exp' variable to \"haar\" or \"haralick\".\n",
    "    - Ensure the 'importance_analysis/importance_[exp]/' directory exists.\n",
    "    - The script expects CSV files ending with the specified 'target' (e.g., CSM).\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835ab770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60179557",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Impossibile trovare il percorso specificato: 'importance_analysis\\\\importance_haar\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_48512/3208929053.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Get the list of CSV files in the folder, sorted alphabetically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcsv_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mnfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Impossibile trovare il percorso specificato: 'importance_analysis\\\\importance_haar\\\\'"
     ]
    }
   ],
   "source": [
    "# Initialization and configuration\n",
    "exp = \"haar\"\n",
    "folder_path = 'importance_analysis' + os.sep + 'importance_' + exp + os.sep\n",
    "target = \"CSM\"\n",
    "threshold = 0.0001\n",
    "feat2show = 20\n",
    "\n",
    "# Get the list of CSV files in the folder, sorted alphabetically\n",
    "csv_files = sorted([f for f in os.listdir(folder_path) if f.endswith(target + '.csv')])\n",
    "print(csv_files)\n",
    "nfiles = len(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c46e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture statistics for each file\n",
    "get_ipython().run_cell_magic('capture', 'cap --no-stderr', \n",
    "'print(f\"Importance Threshold : {threshold}\")\\n'\n",
    "'stats = {}\\n\\n'\n",
    "'for i in range(nfiles):\\n'\n",
    "'    current_file = folder_path + csv_files[i]\\n\\n'\n",
    "'    print(\"Statistics on \" + csv_files[i] + \"\\\\n-------------------------------------------------\\\\n\")\\n'\n",
    "'    \\n'\n",
    "'    data = pd.read_csv(current_file).sort_values(\"importance\", ascending=False)\\n'\n",
    "'    print(f\"First {feat2show} features by importance\")\\n'\n",
    "'    best = data.head(feat2show)\\n'\n",
    "'    print(best)\\n'\n",
    "'    print(\"\\\\n\")\\n'\n",
    "'    print(f\"Best {feat2show} {exp.upper()} features by importance\")\\n'\n",
    "'    # Remove aggregate \"Total\" bands\\n'\n",
    "'    nobands = data.drop(data[data[\"feature\"].str.contains(\"Total\")].index)\\n'\n",
    "'    \\n'\n",
    "'    print(nobands.head(feat2show))\\n'\n",
    "'    \\n'\n",
    "'    print(\"\\\\n\")\\n'\n",
    "'    over_thres = nobands[nobands[\"importance\"] >= threshold]\\n'\n",
    "'    print(f\"{exp.upper()} features over threshold {threshold}: {over_thres.shape[0]}\")\\n'\n",
    "'    print(over_thres)\\n'\n",
    "'    \\n'\n",
    "'    # Save results in dictionary\\n'\n",
    "'    stats[csv_files[i]] = {\"importance\": data, \"importance_best\": data.head(feat2show), '\n",
    "'                           \"nobands_importance\": nobands, \\n'\n",
    "'                           \"nobands_best\": nobands.head(feat2show), \"nobands_ot\": over_thres}\\n'\n",
    "'    print(\"\\\\n\\\\n\")\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6dda974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save general statistics to a text file\n",
    "with open('general_stats.' + exp.upper() + '_' + target + '.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76a94dcb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common HAAR features among experiments (best 20)\n",
      "                         N occurrences  % occurrences    Mean  Best  Worst\n",
      "7080norm_haar_2x_8                  10          100.0   3.500     1      9\n",
      "7080norm_haar_2x_7                  10          100.0   4.000     1      8\n",
      "7080norm_haar_2y_4_flip             10          100.0   9.600     1     19\n",
      "7080norm_haar_3x_4                   9           90.0  14.000    10     19\n",
      "2130norm_haar_2x_8_flip              8           80.0   2.625     1      7\n",
      "...                                ...            ...     ...   ...    ...\n",
      "7080norm_haar_2y_6                   1           10.0  19.000    19     19\n",
      "0512norm_haar_2x_8_flip              1           10.0  19.000    19     19\n",
      "4050norm_haar_2x_6                   1           10.0  20.000    20     20\n",
      "4050norm_haar_3x_4                   1           10.0  20.000    20     20\n",
      "1216norm_haar_2x_4_flip              1           10.0  20.000    20     20\n",
      "\n",
      "[69 rows x 5 columns]\n",
      "\n",
      "\n",
      "Common HAAR features among experiments (best 30)\n",
      "                         N occurrences  % occurrences    Mean  Best  Worst\n",
      "7080norm_haar_2x_8                  10          100.0   3.500     1      9\n",
      "7080norm_haar_2x_7                  10          100.0   4.000     1      8\n",
      "7080norm_haar_2y_4_flip             10          100.0   9.600     1     19\n",
      "7080norm_haar_3x_4                  10          100.0  15.300    10     27\n",
      "2130norm_haar_2x_8_flip              8           80.0   2.625     1      7\n",
      "...                                ...            ...     ...   ...    ...\n",
      "2130norm_haar_4_4_flip               1           10.0  30.000    30     30\n",
      "2130norm_haar_4_4                    1           10.0  30.000    30     30\n",
      "3540norm_haar_3x_2                   1           10.0  30.000    30     30\n",
      "1216norm_haar_2x_3_flip              1           10.0  30.000    30     30\n",
      "7080norm_haar_3x_2_flip              1           10.0  30.000    30     30\n",
      "\n",
      "[110 rows x 5 columns]\n",
      "\n",
      "\n",
      "Common HAAR features among experiments (best 40)\n",
      "                          N occurrences  % occurrences       Mean  Best  Worst\n",
      "7080norm_haar_2x_8                   10          100.0   3.500000     1      9\n",
      "7080norm_haar_2x_7                   10          100.0   4.000000     1      8\n",
      "7080norm_haar_2y_4_flip              10          100.0   9.600000     1     19\n",
      "7080norm_haar_3x_4                   10          100.0  15.300000    10     27\n",
      "7080norm_haar_2x_5                    9           90.0  16.333333     4     37\n",
      "...                                 ...            ...        ...   ...    ...\n",
      "3540norm_haar_2x_7                    1           10.0  38.000000    38     38\n",
      "3540norm_haar_2x_6_flip               1           10.0  38.000000    38     38\n",
      "0512norm_haar_2y_12_flip              1           10.0  39.000000    39     39\n",
      "3540norm_haar_3x_4_flip               1           10.0  39.000000    39     39\n",
      "6070norm_haar_2x_7                    1           10.0  40.000000    40     40\n",
      "\n",
      "[139 rows x 5 columns]\n",
      "\n",
      "\n",
      "Common HAAR features among experiments (best 50)\n",
      "                         N occurrences  % occurrences  Mean  Best  Worst\n",
      "7080norm_haar_2x_8                  10          100.0   3.5     1      9\n",
      "7080norm_haar_2x_7                  10          100.0   4.0     1      8\n",
      "7080norm_haar_2y_4_flip             10          100.0   9.6     1     19\n",
      "7080norm_haar_3x_4                  10          100.0  15.3    10     27\n",
      "7080norm_haar_3x_3_flip             10          100.0  19.6     3     47\n",
      "...                                ...            ...   ...   ...    ...\n",
      "6070norm_haar_2x_6                   1           10.0  50.0    50     50\n",
      "6070norm_haar_3x_2                   1           10.0  50.0    50     50\n",
      "2130norm_haar_2x_2_flip              1           10.0  50.0    50     50\n",
      "5060norm_haar_2x_8                   1           10.0  50.0    50     50\n",
      "3540norm_haar_2x_8_flip              1           10.0  50.0    50     50\n",
      "\n",
      "[172 rows x 5 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze common occurrences across different \"best N\" thresholds\n",
    "full_interlist = {}\n",
    "outf = open('occurrences_stats.' + exp.upper() + '_' + target + '.txt', 'w')\n",
    "\n",
    "for n in [feat2show, feat2show+10, feat2show+20, feat2show+30]:\n",
    "    print(f\"Common {exp.upper()} features among experiments (best {n})\")\n",
    "    outf.write(f\"Common {exp.upper()} features among experiments (best {n})\\n\")\n",
    "    interlist = []\n",
    "    best_dict = {}\n",
    "    imp_dict = {}\n",
    "    std_dict = {}\n",
    "    \n",
    "    # Collect the top features for each experiment\n",
    "    for k, v in stats.items():\n",
    "        band_name = k.split(\"_\", 2)[-1].split(\".\")[0]\n",
    "                                                           \n",
    "        curr_imp_full = v[\"nobands_importance\"]\n",
    "                                                         \n",
    "        curr_best_feat = curr_imp_full[\"feature\"].head(n).to_numpy()\n",
    "        best_dict[band_name] = curr_best_feat\n",
    "\n",
    "        curr_best_imp = curr_imp_full[\"importance\"].head(n).to_numpy()\n",
    "        imp_dict[band_name] = curr_best_imp\n",
    "        \n",
    "        curr_best_std = curr_imp_full[\"std\"].head(n).to_numpy()\n",
    "        std_dict[band_name] = curr_best_std\n",
    "        \n",
    "        \n",
    "        for el in curr_best_feat:\n",
    "            if el not in interlist:\n",
    "                interlist.append(el)\n",
    "                \n",
    "    full_interlist[\"nf_\" + str(n)] = interlist\n",
    "    rank_dict = {}\n",
    "    for h in best_dict.keys():\n",
    "        rank_dict[\"Rank_\" + h] = []\n",
    "        rank_dict[\"Imp_\" + h] = []\n",
    "        rank_dict[\"Std_\" + h] = []\n",
    "\n",
    "    feature_list = []\n",
    "    # INTERLIST: Collection of all unique HAAR/HARALICK features across all bands \n",
    "    # within the top N features, with no repetitions.\n",
    "\n",
    "    for el in interlist:\n",
    "        if el not in feature_list:\n",
    "            feature_list.append(el)        \n",
    "        for b, f in best_dict.items():\n",
    "            pos = np.where(f == el)[0]\n",
    "            \n",
    "                             \n",
    "            if len(pos) > 0:\n",
    "                f_pos = pos[0] + 1\n",
    "                imp = imp_dict[b][pos][0]\n",
    "                s = std_dict[b][pos][0]\n",
    "                                 \n",
    "            else:\n",
    "                # Use -999 as a placeholder for missing features in a specific band\n",
    "                f_pos = -999\n",
    "                imp = -999\n",
    "                s = -999\n",
    "            rank_dict[\"Rank_\" + b].append(f_pos)\n",
    "            rank_dict[\"Imp_\" + b].append(imp)\n",
    "            rank_dict[\"Std_\" + b].append(s)\n",
    "\n",
    "                     \n",
    "    # Create ranking DataFrame\n",
    "    rank_df = pd.DataFrame(rank_dict, index=feature_list)\n",
    "    for b in best_dict.keys():\n",
    "        rank_df.astype({\"Rank_\" + b: 'int16'})\n",
    "    \n",
    "    # Replace placeholders with empty strings for cleaner CSV output\n",
    "    rank_df = rank_df.replace(-999, \"\")\n",
    "    rank_df.to_csv(\"feature_ranking_best\" + str(n) + \"_\" + exp.upper() + \"_\" + target + \".csv\")\n",
    "\n",
    "    # Calculate occurrence statistics\n",
    "                        \n",
    "    occ_dict = {}\n",
    "    for i, f in enumerate(feature_list):\n",
    "        curr_rank = []\n",
    "        for k, v in rank_dict.items():\n",
    "            if k.startswith(\"Rank\"):\n",
    "                if v[i] != -999:\n",
    "                    curr_rank.append(v[i])\n",
    "        occ_dict[f] = curr_rank\n",
    "\n",
    "    stats_dict = {\"N occurrences\": [], \"% occurrences\": [], \"Mean\": [], \"Best\": [], \"Worst\": []}\n",
    "    for f, l in occ_dict.items():\n",
    "        n_occurrence = len(l)\n",
    "        n_perc = n_occurrence / nfiles * 100\n",
    "        mean_pos = np.mean(l)\n",
    "        min_pos = np.min(l)\n",
    "        max_pos = np.max(l)\n",
    "        stats_dict[\"N occurrences\"].append(n_occurrence)\n",
    "        stats_dict[\"% occurrences\"].append(n_perc)\n",
    "        stats_dict[\"Mean\"].append(mean_pos)\n",
    "        stats_dict[\"Best\"].append(min_pos)\n",
    "        stats_dict[\"Worst\"].append(max_pos)\n",
    "        \n",
    "    # Sort results by frequency (descending) and then by average rank (ascending)\n",
    "    stats_df = pd.DataFrame(stats_dict, index=occ_dict.keys())      \n",
    "    stats_df = stats_df.sort_values(by=['N occurrences', 'Mean'], ascending=[False, True])\n",
    "    print(stats_df)\n",
    "    \n",
    "    # Save statistics to text and CSV\n",
    "    outf.write(stats_df.to_string() + \"\\n\\n\")\n",
    "    stats_df.to_csv('occurrences_stats' + str(n) + exp.upper() + '_' + target + '.csv')\n",
    "    print(\"\\n\")\n",
    "\n",
    "outf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a510f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f736b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714026f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
